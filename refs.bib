@Article{garrigos2023handbook,
  title        = "Handbook of convergence theorems for (stochastic)
                 gradient methods",
  author       = "Guillaume Garrigos and Robert M Gower",
  journal      = "arXiv preprint arXiv:2301.11235",
  year         = "2023",
}

@InProceedings{malitsky2020descent,
  author       = "Yura Malitsky and Konstantin Mishchenko",
  publisher    = "PMLR",
  booktitle    = "Proceedings of the 37th International Conference on
                 Machine Learning, {ICML} 2020, 13-18 July 2020, Virtual
                 Event",
  year         = "2020",
  pages        = "6702--6712",
  series       = "Proceedings of Machine Learning Research",
  title        = "Adaptive Gradient Descent without Descent",
  volume       = "119",
}

@Misc{khaled2020better,
  title        = "Better Theory for {SGD} in the Nonconvex World",
  author       = "Ahmed Khaled and Peter Richt√°rik",
  year         = "2020",
  eprint       = "2002.03329",
  archiveprefix = "arXiv",
  primaryclass = "math.OC",
}

@Article{dai2006computational,
  year         = "2006",
  publisher    = "Springer Science and Business Media {LLC}",
  volume       = "33",
  number       = "1",
  author       = "Y. Dai and X. Yang",
  title        = "A New Gradient Method with an Optimal Stepsize
                 Property",
  journal      = "Computational Optimization and Applications",
}

@InProceedings{yair2022parameter,
  author       = "Yair Carmon and Oliver Hinder",
  editor       = "Po{-}Ling Loh and Maxim Raginsky",
  title        = "Making {SGD} Parameter-Free",
  booktitle    = "COLT 2022",
  publisher    = "{PMLR}",
  year         = "2022",
}

@Article{barzilai1988two,
  title        = "Two-point step size gradient methods",
  author       = "Jonathan Barzilai and Jonathan M Borwein",
  journal      = "IMA journal of numerical analysis",
  volume       = "8",
  number       = "1",
  pages        = "141--148",
  year         = "1988",
  publisher    = "Oxford University Press",
}

@Article{bertsekas1997nonlinear,
  title        = "Nonlinear programming",
  author       = "Dimitri P Bertsekas",
  journal      = "Journal of the Operational Research Society",
  volume       = "48",
  number       = "3",
  pages        = "334--334",
  year         = "1997",
  publisher    = "Taylor \& Francis",
}

@InProceedings{cohen2021stability,
  author       = "Jeremy Cohen and Simran Kaur and Yuanzhi Li and J.
                 Zico Kolter and Ameet Talwalkar",
  title        = "Gradient Descent on Neural Networks Typically Occurs
                 at the Edge of Stability",
  booktitle    = "9th International Conference on Learning
                 Representations, {ICLR} 2021, Virtual Event, Austria,
                 May 3-7, 2021",
  publisher    = "OpenReview.net",
  year         = "2021",
}

@InProceedings{ahn2022understanding,
  author       = "Kwangjun Ahn and Jingzhao Zhang and Suvrit Sra",
  editor       = "Kamalika Chaudhuri and Stefanie Jegelka and Le Song
                 and Csaba Szepesv{\'{a}}ri and Gang Niu and Sivan
                 Sabato",
  title        = "Understanding the unstable convergence of gradient
                 descent",
  booktitle    = "International Conference on Machine Learning, {ICML}
                 2022, 17-23 July 2022, Baltimore, Maryland, {USA}",
  series       = "Proceedings of Machine Learning Research",
  volume       = "162",
  pages        = "247--257",
  publisher    = "{PMLR}",
  year         = "2022",
}

@InCollection{bengio2012pratical,
  author       = "Yoshua Bengio",
  editor       = "Gr{\'{e}}goire Montavon and Genevieve B. Orr and
                 Klaus{-}Robert M{\"{u}}ller",
  title        = "Practical Recommendations for Gradient-Based Training
                 of Deep Architectures",
  booktitle    = "Neural Networks: Tricks of the Trade - Second
                 Edition",
  series       = "Lecture Notes in Computer Science",
  volume       = "7700",
  pages        = "437--478",
  publisher    = "Springer",
  year         = "2012",
}

@InProceedings{li2020reconciling,
  author       = "Zhiyuan Li and Kaifeng Lyu and Sanjeev Arora",
  editor       = "Hugo Larochelle and Marc'Aurelio Ranzato and Raia
                 Hadsell and Maria{-}Florina Balcan and Hsuan{-}Tien
                 Lin",
  title        = "Reconciling Modern Deep Learning with Traditional
                 Optimization Analyses: The Intrinsic Learning Rate",
  booktitle    = "Advances in Neural Information Processing Systems 33:
                 Annual Conference on Neural Information Processing
                 Systems 2020, {NeurIPS 2020}, December 6-12, 2020,
                 virtual",
  year         = "2020",
}

@Article{zhang2020first,
  title        = "First-order algorithms without Lipschitz gradient: A
                 sequential local optimization approach",
  author       = "Junyu Zhang and Mingyi Hong",
  journal      = "arXiv preprint arXiv:2010.03194",
  year         = "2020",
}

@Article{patel2022gradient,
  title        = "Gradient descent in the absence of global Lipschitz
                 continuity of the gradients: Convergence, divergence
                 and limitations of its continuous approximation",
  author       = "Vivak Patel and Albert S. Berahas",
  journal      = "arXiv preprint arXiv:2210.02418",
  year         = "2022",
}

@Article{park2021preconditioned,
  title        = "Preconditioned accelerated gradient descent methods
                 for locally Lipschitz smooth objectives with
                 applications to the solution of nonlinear {PDEs}",
  author       = "Jea-Hyun Park and Abner J Salgado and Steven M Wise",
  journal      = "Journal of Scientific Computing",
  volume       = "89",
  number       = "1",
  pages        = "17",
  year         = "2021",
  publisher    = "Springer",
}

@Article{lu2023accelerated,
  title        = "Accelerated first-order methods for convex
                 optimization with locally Lipschitz continuous
                 gradient",
  author       = "Zhaosong Lu and Sanyou Mei",
  journal      = "SIAM Journal on Optimization",
  volume       = "33",
  number       = "3",
  pages        = "2275--2310",
  year         = "2023",
  publisher    = "SIAM",
}

@InProceedings{vladarean2021adaptivity,
  author       = "Maria{-}Luiza Vladarean and Yura Malitsky and Volkan
                 Cevher",
  editor       = "Marc'Aurelio Ranzato and Alina Beygelzimer and Yann N.
                 Dauphin and Percy Liang and Jennifer Wortman Vaughan",
  title        = "A first-order primal-dual method with adaptivity to
                 local smoothness",
  booktitle    = "Advances in Neural Information Processing Systems 34:
                 Annual Conference on Neural Information Processing
                 Systems 2021, {NeurIPS 2021}, December 6-14, 2021,
                 virtual",
  pages        = "6171--6182",
  year         = "2021",
}

@Article{zhao2024adaptive,
  title        = "Adaptive stepsize estimation based accelerated
                 gradient descent algorithm for fully complex-valued
                 neural networks",
  author       = "Weijing Zhao and He Huang",
  journal      = "Expert Systems with Applications",
  volume       = "236",
  pages        = "121166",
  year         = "2024",
  publisher    = "Elsevier",
}

@InProceedings{vainsencher2015local,
  author       = "Daniel Vainsencher and Han Liu and Tong Zhang",
  editor       = "Corinna Cortes and Neil D. Lawrence and Daniel D. Lee
                 and Masashi Sugiyama and Roman Garnett",
  title        = "Local Smoothness in Variance Reduced Optimization",
  booktitle    = "Advances in Neural Information Processing Systems 28:
                 Annual Conference on Neural Information Processing
                 Systems 2015, December 7-12, 2015, Montreal, Quebec,
                 Canada",
  pages        = "2179--2187",
  year         = "2015",
}

@Article{zhang2013weaker,
  author       = "Hui Zhang and Wotao Yin",
  title        = "Gradient methods for convex minimization: better rates
                 under weaker conditions",
  journal      = "CoRR",
  volume       = "abs/1303.4645",
  year         = "2013",
}

@Article{curtis2021regional,
  author       = "Frank E. Curtis and Daniel P. Robinson",
  title        = "Regional complexity analysis of algorithms for
                 nonconvex smooth optimization",
  journal      = "Math. Program.",
  volume       = "187",
  number       = "1",
  pages        = "579--615",
  year         = "2021",
}

@InProceedings{guille2021condition,
  author       = "Charles Guille{-}Escuret and Manuela Girotti and
                 Baptiste Goujaud and Ioannis Mitliagkas",
  editor       = "Arindam Banerjee and Kenji Fukumizu",
  title        = "A Study of Condition Numbers for First-Order
                 Optimization",
  booktitle    = "The 24th International Conference on Artificial
                 Intelligence and Statistics, {AISTATS} 2021, April
                 13-15, 2021, Virtual Event",
  series       = "Proceedings of Machine Learning Research",
  volume       = "130",
  pages        = "1261--1269",
  publisher    = "{PMLR}",
  year         = "2021",
}

@Book{nesterov2018lectures,
  title        = "Lectures on convex optimization",
  author       = "Yurii Nesterov and others",
  volume       = "137",
  year         = "2018",
  publisher    = "Springer",
}

@InProceedings{pan21_eigen,
  author       = "Rui Pan and Haishan Ye and Tong Zhang",
  publisher    = "OpenReview.net",
  booktitle    = "{ICLR}",
  year         = "2022",
  title        = "Eigencurve: Optimal Learning Rate Schedule for {SGD}
                 on Quadratic Objectives with Skewed Hessian Spectrums",
}

@InProceedings{karimi2016linear,
  title        = "Linear convergence of gradient and proximal-gradient
                 methods under the polyak-{\l}ojasiewicz condition",
  author       = "Hamed Karimi and Julie Nutini and Mark Schmidt",
  booktitle    = "Machine Learning and Knowledge Discovery in Databases:
                 European Conference, {ECML} {PKDD 2016}, Riva del
                 Garda, Italy, September 19-23, 2016, Proceedings, Part
                 {I 16}",
  pages        = "795--811",
  year         = "2016",
  organization = "Springer",
}

@Article{paquette2023halting,
  title        = "Halting time is predictable for large models: A
                 universality property and average-case analysis",
  author       = "Courtney Paquette and Bart van Merri{\"e}nboer and
                 Elliot Paquette and Fabian Pedregosa",
  journal      = "Foundations of Computational Mathematics",
  volume       = "23",
  number       = "2",
  pages        = "597--673",
  year         = "2023",
  publisher    = "Springer",
}

@Article{cohen22_adapt_gradien_method_at_edge_stabil,
  author       = "Jeremy M. Cohen and Behrooz Ghorbani and Shankar
                 Krishnan and Naman Agarwal and Sourabh Medapati and
                 Michal Badura and Daniel Suo and David Cardoze and
                 Zachary Nado and George E. Dahl and Justin Gilmer",
  year         = "2022",
  journal      = "arXiv preprint arXiv:2207.14484",
  title        = "Adaptive Gradient Methods At the Edge of Stability",
  volume       = "abs/2207.14484",
}

@Article{berahas23_non_unifor_smoot_gradien_descen,
  author       = "Albert S. Berahas and Lindon Roberts and Fred Roosta",
  year         = "2023",
  journal      = "arXiv preprint arXiv:2311.08615",
  title        = "Non-Uniform Smoothness for Gradient Descent",
  volume       = "abs/2311.08615",
}

@InProceedings{mei21_lever_non_unifor_first_order,
  author       = "Jincheng Mei and Yue Gao and Bo Dai and Csaba
                 Szepesv√°ri and Dale Schuurmans",
  editor       = "Marina Meila and Tong Zhang",
  publisher    = "PMLR",
  booktitle    = "{ICML} 2021",
  year         = "2021",
  title        = "Leveraging Non-uniformity in First-order Non-convex
                 Optimization",
}

@Book{beck17_first_order_methods_opt,
  author       = "Amir Beck",
  location     = "Philadelphia : Philadelphia",
  publisher    = "Society for Industrial and Applied Mathematics ;
                 Mathematical Optimization Society",
  year         = "2017",
  ISBN         = "978-161-197-4-9-9-7",
  keywords     = "Mathematical optimization,Convergence",
  series       = "{MOS}-{SIAM} series on optimization",
  title        = "First-order methods in optimization",
}

@Article{polyak1987introduction,
  title        = "Introduction to optimization",
  author       = "Boris T Polyak",
  year         = "1987",
  publisher    = "New York, Optimization Software,",
}

@Article{hazan2019revisiting,
  title        = "Revisiting the Polyak step size",
  author       = "Elad Hazan and Sham Kakade",
  journal      = "arXiv preprint arXiv:1905.00313",
  year         = "2019",
}

@Article{altschuler2023hedging,
  author       = "Jason M. Altschuler and Pablo A. Parrilo",
  title        = "Acceleration by Stepsize Hedging {I:} Multi-Step
                 Descent and the Silver Stepsize Schedule",
  journal      = "CoRR",
  volume       = "abs/2309.07879",
  year         = "2023",
}

@Article{hogan1973point,
  title        = "Point-to-set maps in mathematical programming",
  author       = "William W Hogan",
  journal      = "SIAM review",
  volume       = "15",
  number       = "3",
  pages        = "591--603",
  year         = "1973",
  publisher    = "SIAM",
}

@Misc{asuncion2007uci,
  title        = "{UCI} machine learning repository",
  author       = "Arthur Asuncion and David Newman",
  year         = "2007",
  publisher    = "Irvine, CA, USA",
}

@InProceedings{duchi11_adagrad,
  author       = "John C. Duchi and Elad Hazan and Yoram Singer",
  editor       = "Adam Tauman Kalai and Mehryar Mohri",
  publisher    = "Omnipress",
  booktitle    = "{COLT} 2010 - The 23rd Conference on Learning Theory,
                 Haifa, Israel, June 27-29, 2010",
  year         = "2010",
  pages        = "257--269",
  title        = "Adaptive Subgradient Methods for Online Learning and
                 Stochastic Optimization",
}

@Article{streeter10_less_regret_via_onlin_condit,
  abstract     = "We analyze and evaluate an online gradient descent
                 algorithm with adaptive per-coordinate adjustment of
                 learning rates. Our algorithm can be thought of as an
                 online version of batch gradient descent with a
                 diagonal preconditioner. This approach leads to regret
                 bounds that are stronger than those of standard online
                 gradient descent for general online convex optimization
                 problems. Experimentally, we show that our algorithm is
                 competitive with state-of-the-art algorithms for large
                 scale machine learning problems.",
  author       = "Matthew Streeter and H. Brendan McMahan",
  year         = "2010",
  eprintclass  = "cs.LG",
  eprinttype   = "arXiv",
  journal      = "arXiv preprint arXiv:1002.4862",
  title        = "Less Regret Via Online Conditioning",
}

@Article{orabona23_normal_gradien_all,
  author       = "Francesco Orabona",
  year         = "2023",
  journal      = "arXiv preprint arXiv:2308.05621",
  title        = "Normalized Gradients for All",
  volume       = "abs/2308.05621",
}

@InProceedings{levy17_onlin_to_offlin_conver_univer,
  author       = "Kfir Y. Levy",
  editor       = "Isabelle Guyon and Ulrike von Luxburg and Samy Bengio
                 and Hanna M. Wallach and Rob Fergus and S. V. N.
                 Vishwanathan and Roman Garnett",
  booktitle    = "Advances in Neural Information Processing Systems 30:
                 Annual Conference on Neural Information Processing
                 Systems 2017, December 4-9, 2017, Long Beach, {CA},
                 {USA}",
  year         = "2017",
  pages        = "1613--1622",
  title        = "Online to Offline Conversions, Universality and
                 Adaptive Minibatch Sizes",
}

@Article{pan22_adam_sgd,
  author       = "Yan Pan and Yuanzhi Li",
  year         = "2022",
  journal      = "OPT2023: 14th Annual Workshop on Optimization for
                 Machine Learning",
  title        = "Toward Understanding Why Adam Converges Faster Than
                 {SGD} for Transformers",
}

@Article{paszke2019pytorch,
  title        = "Pytorch: An imperative style, high-performance deep
                 learning library",
  author       = "Adam Paszke and Sam Gross and Francisco Massa and Adam
                 Lerer and James Bradbury and Gregory Chanan and Trevor
                 Killeen and Zeming Lin and Natalia Gimelshein and Luca
                 Antiga and others",
  journal      = "Advances in neural information processing systems",
  volume       = "32",
  year         = "2019",
}

@Article{fernandez2014we,
  title        = "Do we need hundreds of classifiers to solve real world
                 classification problems?",
  author       = "Manuel Fern{\'a}ndez-Delgado and Eva Cernadas and
                 Sen{\'e}n Barro and Dinani Amorim",
  journal      = "The journal of machine learning research",
  volume       = "15",
  number       = "1",
  pages        = "3133--3181",
  year         = "2014",
  publisher    = "JMLR. org",
}

@InProceedings{he2015delving,
  title        = "Delving deep into rectifiers: Surpassing human-level
                 performance on imagenet classification",
  author       = "Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian
                 Sun",
  booktitle    = "Proceedings of the {IEEE} international conference on
                 computer vision",
  pages        = "1026--1034",
  year         = "2015",
}

@Article{2020SciPy-NMeth,
  author       = "Pauli Virtanen and Ralf Gommers and Travis E. Oliphant
                 and Matt Haberland and Tyler Reddy and David Cournapeau
                 and Evgeni Burovski and Pearu Peterson and Warren
                 Weckesser and Jonathan Bright and St{\'e}fan J. {van
                 der Walt} and Matthew Brett and Joshua Wilson and K.
                 Jarrod Millman and Nikolay Mayorov and Andrew R. J.
                 Nelson and Eric Jones and Robert Kern and Eric Larson
                 and C J Carey and {\.I}lhan Polat and Yu Feng and Eric
                 W. Moore and Jake {VanderPlas} and Denis Laxalde and
                 Josef Perktold and Robert Cimrman and Ian Henriksen and
                 E. A. Quintero and Charles R. Harris and Anne M.
                 Archibald and Ant{\^o}nio H. Ribeiro and Fabian
                 Pedregosa and Paul {van Mulbregt} and {SciPy 1.0
                 Contributors}",
  title        = "{{SciPy} 1.0: Fundamental Algorithms for Scientific
                 Computing in Python}",
  journal      = "Nature Methods",
  year         = "2020",
  volume       = "17",
  pages        = "261--272",
}

@Article{bubeck2015convex,
  title        = "Convex optimization: Algorithms and complexity",
  author       = "S{\'e}bastien Bubeck and others",
  journal      = "Foundations and Trends{\textregistered} in Machine
                 Learning",
  volume       = "8",
  number       = "3-4",
  pages        = "231--357",
  year         = "2015",
  publisher    = "Now Publishers, Inc.",
}

@Article{liu1989limited,
  title        = "On the limited memory {BFGS} method for large scale
                 optimization",
  author       = "Dong C Liu and Jorge Nocedal",
  journal      = "Mathematical programming",
  volume       = "45",
  number       = "1-3",
  pages        = "503--528",
  year         = "1989",
  publisher    = "Springer",
}

@Article{grimmer2019convergence,
  author       = "Benjamin Grimmer",
  title        = "Convergence Rates for Deterministic and Stochastic
                 Subgradient Methods without Lipschitz Continuity",
  journal      = "{SIAM} J. Optim.",
  volume       = "29",
  number       = "2",
  pages        = "1350--1365",
  year         = "2019",
}

@InProceedings{nesterov1983method,
  title        = "A method for solving the convex programming problem
                 with convergence rate {O} (1/k2)",
  author       = "Yurii Nesterov",
  booktitle    = "Dokl akad nauk Sssr",
  volume       = "269",
  pages        = "543",
  year         = "1983",
}

@Article{mishkin2024faster,
  title        = "Faster Convergence of Stochastic Accelerated Gradient
                 Descent under Interpolation",
  author       = "Aaron Mishkin and Mert Pilanci and Mark Schmidt",
  journal      = "arXiv preprint arXiv:2404.02378",
  year         = "2024",
}

@InProceedings{zhang2020clipping,
  author       = "Jingzhao Zhang and Tianxing He and Suvrit Sra and Ali
                 Jadbabaie",
  title        = "Why Gradient Clipping Accelerates Training: {A}
                 Theoretical Justification for Adaptivity",
  booktitle    = "8th International Conference on Learning
                 Representations, {ICLR} 2020, Addis Ababa, Ethiopia,
                 April 26-30, 2020",
  publisher    = "OpenReview.net",
  year         = "2020",
}

@InProceedings{zhang2020improved,
  author       = "Bohang Zhang and Jikai Jin and Cong Fang and Liwei
                 Wang",
  editor       = "Hugo Larochelle and Marc'Aurelio Ranzato and Raia
                 Hadsell and Maria{-}Florina Balcan and Hsuan{-}Tien
                 Lin",
  title        = "Improved Analysis of Clipping Algorithms for
                 Non-convex Optimization",
  booktitle    = "Advances in Neural Information Processing Systems 33:
                 Annual Conference on Neural Information Processing
                 Systems 2020, {NeurIPS 2020}, December 6-12, 2020,
                 virtual",
  year         = "2020",
}

@InProceedings{li2023generalized,
  author       = "Haochuan Li and Jian Qian and Yi Tian and Alexander
                 Rakhlin and Ali Jadbabaie",
  editor       = "Alice Oh and Tristan Naumann and Amir Globerson and
                 Kate Saenko and Moritz Hardt and Sergey Levine",
  title        = "Convex and Non-convex Optimization Under Generalized
                 Smoothness",
  booktitle    = "Advances in Neural Information Processing Systems 36:
                 Annual Conference on Neural Information Processing
                 Systems 2023, {NeurIPS 2023}, New Orleans, {LA}, {USA},
                 December 10 - 16, 2023",
  year         = "2023",
}

@Article{takezawa2024polyak,
  author       = "Yuki Takezawa and Han Bao and Ryoma Sato and Kenta
                 Niwa and Makoto Yamada",
  title        = "Polyak Meets Parameter-free Clipped Gradient Descent",
  journal      = "CoRR",
  volume       = "abs/2405.15010",
  year         = "2024",
  URL          = "https://doi.org/10.48550/arXiv.2405.15010",
  DOI          = "10.48550/ARXIV.2405.15010",
  eprinttype   = "arXiv",
  eprint       = "2405.15010",
}

@InProceedings{ahn2022unstable,
  author       = "Kwangjun Ahn and Jingzhao Zhang and Suvrit Sra",
  editor       = "Kamalika Chaudhuri and Stefanie Jegelka and Le Song
                 and Csaba Szepesv{\'{a}}ri and Gang Niu and Sivan
                 Sabato",
  title        = "Understanding the unstable convergence of gradient
                 descent",
  booktitle    = "International Conference on Machine Learning, {ICML}
                 2022, 17-23 July 2022, Baltimore, Maryland, {USA}",
  series       = "Proceedings of Machine Learning Research",
  volume       = "162",
  pages        = "247--257",
  publisher    = "{PMLR}",
  year         = "2022",
}

@Article{pan2023toward,
  title        = "Toward understanding why adam converges faster than
                 sgd for transformers",
  author       = "Yan Pan and Yuanzhi Li",
  journal      = "arXiv preprint arXiv:2306.00204",
  year         = "2023",
}
